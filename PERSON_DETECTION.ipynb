{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vQBgy_OH6UU",
        "outputId": "59fb4a53-7596-47cd-839e-378fc184126e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.8043417930603027\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Load a pre-trained ResNet model\n",
        "model = models.resnet18(pretrained=True)\n",
        "model = nn.Sequential(*list(model.children())[:-1])  # Remove the final classification layer\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Define a transformation to prepare the image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load and transform the images\n",
        "img1 = Image.open('/content/Screenshot_2024-09-09-22-40-16-633_com.google.android.apps.docs-edit (3)-min (1).jpg')\n",
        "img2 = Image.open('/content/WhatsApp Image 2024-09-10 at 10.07.13 AM.jpeg')\n",
        "\n",
        "img1 = transform(img1).unsqueeze(0)\n",
        "img2 = transform(img2).unsqueeze(0)\n",
        "\n",
        "# Extract features\n",
        "with torch.no_grad():\n",
        "    features1 = model(img1)\n",
        "    features2 = model(img2)\n",
        "\n",
        "# Compare features using cosine similarity\n",
        "cosine_similarity = nn.CosineSimilarity(dim=1)\n",
        "similarity = cosine_similarity(features1, features2)\n",
        "\n",
        "print(f\"Cosine Similarity: {similarity.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Load a pre-trained ResNet model\n",
        "model = models.resnet18(pretrained=True)\n",
        "model = nn.Sequential(*list(model.children())[:-1])  # Remove the final classification layer\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Define a transformation to prepare the image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load and transform the images\n",
        "img1 = Image.open('/content/Screenshot_2024-09-09-22-40-16-633_com.google.android.apps.docs-edit (3)-min (1).jpg')\n",
        "img2 = Image.open('/content/WhatsApp Image 2024-09-10 at 10.07.13 AM.jpeg')\n",
        "\n",
        "img1 = transform(img1).unsqueeze(0)\n",
        "img2 = transform(img2).unsqueeze(0)\n",
        "\n",
        "# Extract features\n",
        "with torch.no_grad():\n",
        "    features1 = model(img1)\n",
        "    features2 = model(img2)\n",
        "\n",
        "# Compare features using cosine similarity\n",
        "cosine_similarity = nn.CosineSimilarity(dim=1)\n",
        "similarity = cosine_similarity(features1, features2)\n",
        "\n",
        "print(f\"Cosine Similarity: {similarity.item()}\")\n"
      ],
      "metadata": {
        "id": "DqTvcnGvJddO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44fc526b-eb5f-40fb-ef7f-6cb92a36861a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.8043417930603027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if (similarity.item() == 1.0):\n",
        "  print(\"same person\")\n",
        "else:\n",
        "    print(\"different persons\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb8EHgVLKLKP",
        "outputId": "218d9523-836c-48a9-f26b-f8cb3cbda2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "different persons\n"
          ]
        }
      ]
    }
  ]
}